{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0bc44fb-faa1-433f-9043-2f180fd46209",
   "metadata": {},
   "source": [
    "# Neural Style Transfer\n",
    "\n",
    "Resources:\n",
    "* [Original paper](https://arxiv.org/abs/1508.06576)\n",
    "* [Official PyTorch Tutorial](https://pytorch.org/tutorials/advanced/neural_style_tutorial.html)\n",
    "* [Blog by Amar](https://towardsdatascience.com/implementing-neural-style-transfer-using-pytorch-fd8d43fb7bfa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08ab1cf-ed0b-40b6-825f-2ff8f80d20f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457c9ab2-8e1d-42e9-bfff-50929ab16d35",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Here we describe vgg19 model.\n",
    "\n",
    "Some image of vgg19.\n",
    "\n",
    "Then its implementation in pytorch and how I am slicing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a07fc01-50e5-42ae-810d-4567b5eca218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG19, self).__init__()\n",
    "        self.model = models.vgg19(pretrained=True)\n",
    "\n",
    "    def forward(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2664e504-063d-43c8-814d-410d691e3fd4",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c390769-fc07-4175-a918-fd023a199232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "047cb19a-5a33-400f-83ef-3786c7805232",
   "metadata": {},
   "source": [
    "### Total Loss\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{total}(\\vec{p}, \\vec{a}, \\vec{x}) = \\alpha\\mathcal{L}_{content}(\\vec{p}, \\vec{x}) + \\beta\\mathcal{L}_{style}(\\vec{a}, \\vec{x}) \\tag{1}\n",
    "$$\n",
    "$ \\vec{p} $ - content image  \n",
    "$ \\vec{a} $ - style image   \n",
    "$ \\vec{x} $ - generated image   \n",
    "$ \\alpha $ - content coefficient   \n",
    "$ \\beta $ - style cooefficient \n",
    "\n",
    "> Generated image $\\vec{x}$ can be either initialized as content image or white noise (random values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4200f4-e33b-4057-ac81-11f19f3a2c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TotalLoss(nn.Module):\n",
    "    def __init__(self, content_features: Tensor, style_features: Tensor, alpha: float = 1., beta: float = 1000.):\n",
    "        super(TotalLoss, self).__init__()\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "        self.content_loss = ContentLoss(content_features)\n",
    "        self.style_loss = StyleLoss(style_features)\n",
    "\n",
    "    def forward(self):\n",
    "        total_loss = self.alpha * self.content_loss + self.beta * self.style_loss\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f02e16-6423-4fc1-abd7-a399082ff7ca",
   "metadata": {},
   "source": [
    "### Content Loss\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{content}(\\vec{p}, \\vec{x}) = \\dfrac{1}{2} \\sum_{i,j}(F_{i,j}^{l} - P_{i,j}^{l})^2 \\tag{2}\n",
    "$$\n",
    "\n",
    "Content loss is a squared error between: \n",
    "  \n",
    "$ F_{i,j}^{l} $ - Output of conv layer **$l$** for input (generated) image  \n",
    "$ P_{i,j}^{l} $ - Output of conv layer **$l$** for content image  \n",
    "\n",
    "> $ i, j $ represent **i-th** position of the filter at position **j** which implementation-wise doesn't change anything as we take whole outputs of conv layers\n",
    "\n",
    "There's a small difference in notation when it comes to implementation. In paper, $F^{l}$ is defined as $F^{l} \\in \\mathbb{R}^{N_l x M_l}$, which means it's a matrix with shape ($N_l$ - number of feature maps, $M_l$ - $width * height$ of feature maps). In contrast, here we have $F^{l} \\in \\mathbb{R}^{N_l x H_l x W_l}$ which is just a reshaped version of the matrix in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93031e9b-9174-416a-871f-c0e27df93abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self, content_features: Tensor):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        \n",
    "        self.mse = nn.MSELoss()\n",
    "        self.content_features = content_features\n",
    "\n",
    "    def forward(self, input_features: Tensor) -> Tensor:\n",
    "        return self.mse(input_features, self.content_features, reduction='sum') / 2  # Matches formula (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9452459e-ebf7-4163-aaf6-f2229c232b93",
   "metadata": {},
   "source": [
    "### Style Loss\n",
    "\n",
    "Instead of taking raw conv layer outputs as in content loss, style loss firstly computes Gram matrix. They mentioned that Gram matrix computes feature correlations between feature maps? (whill have to look more into what that exactly means).\n",
    "$$\n",
    "G_{i,j}^l = \\sum_{k}(F_{i,k}^lF_{j,k}^l) \\tag{3}\n",
    "$$\n",
    "where:\n",
    "\n",
    "$F_{i,k}^l$ is again output of conv layer **$l$**, defined as $F^{l} \\in \\mathbb{R}^{N_l x M_l}$, where $(N_l = channels, M_l = height * width)$  \n",
    "$F_{j,k}^l$ is transposed version of previously mentioned matrix\n",
    "\n",
    "This essentially means $G$ is computed as $F * F^T$ and $G \\in \\mathbb{R}^{NxN}$\n",
    "\n",
    "> Because $G$ is of shape $NxN$, it means that the dimensions of Gram matrix vary between conv layers with different number of feature maps. In official PyTorch tutorial this is resolved by normalazing each gram matrix by dividing it with its number of elements. I haven't seen this in paper but I'll test both **with** and **without** normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46de8b1-26f9-4cbb-a9df-f7cdac3c1300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(feature_maps: Tensor, normalize: bool = False) -> Tensor:\n",
    "    B, C, H, W = feature_maps.size()\n",
    "\n",
    "    assert B == 1, f\"Batch size must be 1! Got B={B}\"\n",
    "\n",
    "    feature_maps = feature_maps.squeeze(0)  # Remove batch_size\n",
    "    features = feature_maps.view(C, H * W)\n",
    "    g = torch.mm(features, features.t())\n",
    "\n",
    "    if normalize:\n",
    "        g = g / g.numel()\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758cb481-7909-484f-9ff0-35ea86f3c35f",
   "metadata": {},
   "source": [
    "With *Gram matrix* defined, they computed the **loss per layer** as mean-squared error:\n",
    "$$\n",
    "E_l = \\dfrac{1}{4N_l^2M_l^2}\\sum_{i,j}(G_{i,j}^l-A_{i,j}^l)^2\\tag{4}\n",
    "$$\n",
    "> How the hell is this mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75e2226-88ec-4d2a-9708-aa85f1836320",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, style_features: List[Tensor]):\n",
    "        super(StyleLoss, self).__init__()\n",
    "\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.style_features = style_features\n",
    "        self.style_gram_matrix = [gram_matrix(style_feature) for style_feature in self.style_features]\n",
    "\n",
    "    @property\n",
    "    def num_layers(self):\n",
    "        return len(self.style_features)  # This will be constant w\n",
    "\n",
    "    def forward(self, input_features: List[Tensor]) -> Tensor:\n",
    "        assert len(input_features) == len(self.style_gram_matrix), \\\n",
    "            f\"Mismatched lengths of features! {len(input_features)} != {len(self.style_features)}\"\n",
    "\n",
    "        inputs_gram_matrix = [gram_matrix(inpute_feature) for inpute_feature in input_features]\n",
    "\n",
    "        style_loss = 0\n",
    "        for style_gram, input_gram in zip(self.style_gram_matrix, inputs_gram_matrix):\n",
    "            e_l = self.mse(input_gram, style_gram)  # input_gram = G, style_gram = A, e_l = El in formula (4)\n",
    "            style_loss += e_l\n",
    "\n",
    "        return style_loss / self.num_layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
